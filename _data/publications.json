[	
	{
		"type": "misc",
		"id": "marjanović-etal-2025-deepseekr1-thoughtology",
		"title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
		"authors": [
		"Sara Vera Marjanović",
		"Arkil Patel",
		"Vaibhav Adlakha",
		"Milad Aghajohari",
		"Parishad BehnamGhader",
		"Mehar Bhatia",
		"Aditi Khandelwal",
		"Austin Kraft",
		"Benno Krojer",
		"Xing Han Lù",
		"Nicholas Meade",
		"Dongchan Shin",
		"Amirhossein Kazemnejad",
		"Gaurav Kamath",
		"Marius Mosbach",
		"Karolina Stańczak",
		"Siva Reddy"
		],
		"year": 2025,
		"eprint": "2504.07128",
		"url": "https://arxiv.org/abs/2504.07128",
		"abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly 'thinking' about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-à-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
		"conference": "Preprint",
		"selected": true,
		"award": "None",
		"code": "None",
		"blogpost": "None"
	},
	{
		"type": "misc",
		"id": "krishnan-etal-2025-data-unlearned-equally",
		"title": "Not All Data Are Unlearned Equally",
		"authors": [
		  "Aravind Krishnan",
		  "Siva Reddy",
		  "Marius Mosbach"
		],
		"year": 2025,
		"url": "https://arxiv.org/abs/2504.05058",
		"abstract": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
		"conference": "Preprint",
		"blogpost": "None",
		"award": "None",
		"code": "https://github.com/McGill-NLP/unequal-unlearning",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "mosbach-etal-2024-insights",
		"title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
		"authors": [
			"Marius Mosbach",
			"Vagrant Gautam",
			"Tomás Vergara Browne",
			"Dietrich Klakow",
			"Mor Geva"
		],
		"editor": ["Yaser Al-Onaizan", "Mohit Bansal", "Yun-Nung Chen"],
		"booktitle": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
		"month": "November",
		"year": 2024,
		"address": "Miami, Florida, USA",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2024.emnlp-main.181/",
		"doi": "10.18653/v1/2024.emnlp-main.181",
		"pages": "3078--3105",
		"abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a criticism of this work is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and their references and citations, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it as important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research.",
		"conference": "EMNLP",
		"award": "None",
		"code": "https://aclanthology.org/2024.emnlp-main.181/",
		"selected": true,
		"blogpost": "None"
	},
	{
		"type": "misc",
		"id": "behnamghader2024llm2veclargelanguagemodels",
		"title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
		"authors": [
			"Parishad BehnamGhader",
			"Vaibhav Adlakha",
			"Marius Mosbach",
			"Dzmitry Bahdanau",
			"Nicolas Chapados",
			"Siva Reddy"
		],
		"year": 2024,
		"eprint": "2404.05961",
		"archivePrefix": "arXiv",
		"primaryClass": "cs.CL",
		"url": "https://arxiv.org/abs/2404.05961",
		"abstract": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.",
		"conference": "COLM",
		"code": "https://github.com/McGill-NLP/llm2vec",
		"blogpost": "https://www.servicenow.com/blogs/2024/llm2vec-large-language-models",
		"award": "None",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "garcia-de-herreros-etal-2024-explains",
		"title": "What explains the success of cross-modal fine-tuning with ORCA?",
		"authors": [
			"Paloma Garcia De Herreros",
			"Vagrant Gautam",
			"Philipp Slusallek",
			"Dietrich Klakow",
			"Marius Mosbach"
		],
		"editor": [
			"Shabnam Tafreshi",
			"Arjun Akula",
			"João Sedoc",
			"Aleksandr Drozd",
			"Anna Rogers",
			"Anna Rumshisky"
		],
		"booktitle": "Proceedings of the Fifth Workshop on Insights from Negative Results in NLP",
		"month": "June",
		"year": 2024,
		"address": "Mexico City, Mexico",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2024.insights-1.2/",
		"doi": "10.18653/v1/2024.insights-1.2",
		"pages": "8--16",
		"abstract": "ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.",
		"conference": "Insights Workshop @ NAACL",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "alabi-etal-2024-hidden",
		"title": "The Hidden Space of Transformer Language Adapters",
		"authors": [
			"Jesujoba Alabi",
			"Marius Mosbach",
			"Matan Eyal",
			"Dietrich Klakow",
			"Mor Geva"
		],
		"editor": ["Lun-Wei Ku", "Andre Martins", "Vivek Srikumar"],
		"booktitle": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"month": "August",
		"year": 2024,
		"address": "Bangkok, Thailand",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2024.acl-long.356/",
		"doi": "10.18653/v1/2024.acl-long.356",
		"pages": "6588--6607",
		"conference": "ACL",
		"blogpost": "None",
		"code": "None",
		"award": "None",
		"abstract": "We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an isolated subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "zhang-etal-2024-impact",
		"title": "The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis",
		"authors": [
			"Miaoran Zhang",
			"Vagrant Gautam",
			"Mingyang Wang",
			"Jesujoba Alabi",
			"Xiaoyu Shen",
			"Dietrich Klakow",
			"Marius Mosbach"
		],
		"editor": ["Lun-Wei Ku", "Andre Martins", "Vivek Srikumar"],
		"booktitle": "Findings of the Association for Computational Linguistics: ACL 2024",
		"month": "August",
		"year": 2024,
		"address": "Bangkok, Thailand",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2024.findings-acl.438/",
		"doi": "10.18653/v1/2024.findings-acl.438",
		"pages": "7342--7371",
		"abstract": "In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.",
		"conference": "ACL (Findings)",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "steuer-etal-2023-large",
		"title": "Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures",
		"authors": ["Julius Steuer", "Marius Mosbach", "Dietrich Klakow"],
		"editor": [
			"Alex Warstadt",
			"Aaron Mueller",
			"Leshem Choshen",
			"Ethan Wilcox",
			"Chengxu Zhuang",
			"Juan Ciro",
			"Rafael Mosquera",
			"Bhargavi Paranjabe",
			"Adina Williams",
			"Tal Linzen",
			"Ryan Cotterell"
		],
		"booktitle": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
		"month": "December",
		"year": 2023,
		"address": "Singapore",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2023.conll-babylm.12/",
		"doi": "10.18653/v1/2023.conll-babylm.12",
		"pages": "142--157",
		"abstract": "Research on the cognitive plausibility of language models (LMs) has so far mostly concentrated on modelling psycholinguistic response variables such as reading times, gaze durations and N400/P600 EEG signals, while mostly leaving out the dimension of what Mahowald et al. (2023) described as formal and functional linguistic competence, and developmental plausibility. We address this gap by training a series of GPT-like language models of different sizes on the strict version of the BabyLM pretraining corpus, evaluating on the challenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction task. We find a positive correlation between LM size and performance on all three challenge tasks, with different preferences for model width and depth in each of the tasks. In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal. This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.",
		"conference": "BabyLM Challenge @ CoNLL",
		"award": "🏆 Most Interesting Paper Award  🏆",
		"code": "None",
		"blogpost": "None",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "mosbach-etal-2023-shot",
		"title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",
		"authors": [
			"Marius Mosbach",
			"Tiago Pimentel",
			"Shauli Ravfogel",
			"Dietrich Klakow",
			"Yanai Elazar"
		],
		"editor": ["Anna Rogers", "Jordan Boyd-Graber", "Naoaki Okazaki"],
		"booktitle": "Findings of the Association for Computational Linguistics: ACL 2023",
		"month": "July",
		"year": 2023,
		"address": "Toronto, Canada",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2023.findings-acl.779/",
		"doi": "10.18653/v1/2023.findings-acl.779",
		"pages": "12284--12314",
		"abstract": "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.",
		"conference": "ACL (Findings)",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "zhu-etal-2023-weaker",
		"title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning",
		"authors": [
			"Dawei Zhu",
			"Xiaoyu Shen",
			"Marius Mosbach",
			"Andreas Stephan",
			"Dietrich Klakow"
		],
		"editor": ["Anna Rogers", "Jordan Boyd-Graber", "Naoaki Okazaki"],
		"booktitle": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"month": "July",
		"year": 2023,
		"address": "Toronto, Canada",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2023.acl-long.796/",
		"doi": "10.18653/v1/2023.acl-long.796",
		"pages": "14229--14253",
		"abstract": "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.",
		"conference": "ACL",
		"code": "None",
		"blogpost": "None",
		"award": "🏆 Best Theme Paper Award 🏆",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "alabi-etal-2022-adapting",
		"title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning",
		"authors": [
			"Jesujoba O. Alabi",
			"David Ifeoluwa Adelani",
			"Marius Mosbach",
			"Dietrich Klakow"
		],
		"editor": [
			"Nicoletta Calzolari",
			"Chu-Ren Huang",
			"Hansaem Kim",
			"James Pustejovsky",
			"Leo Wanner",
			"Key-Sun Choi",
			"Pum-Mo Ryu",
			"Hsin-Hsi Chen",
			"Lucia Donatelli",
			"Heng Ji",
			"Sadao Kurohashi",
			"Patrizia Paggio",
			"Nianwen Xue",
			"Seokhwan Kim",
			"Younggyun Hahm",
			"Zhong He",
			"Tony Kyungil Lee",
			"Enrico Santus",
			"Francis Bond",
			"Seung-Hoon Na"
		],
		"booktitle": "Proceedings of the 29th International Conference on Computational Linguistics",
		"month": "October",
		"year": 2022,
		"address": "Gyeongju, Republic of Korea",
		"publisher": "International Committee on Computational Linguistics",
		"url": "https://aclanthology.org/2022.coling-1.382/",
		"pages": "4336--4349",
		"abstract": "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) — fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
		"conference": "COLING",
		"code": "None",
		"blogpost": "None",
		"award": "🏆 Best Paper Award 🏆",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "zhang-etal-2022-mcse",
		"title": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings",
		"authors": [
			"Miaoran Zhang",
			"Marius Mosbach",
			"David Adelani",
			"Michael Hedderich",
			"Dietrich Klakow"
		],
		"editor": [
			"Marine Carpuat",
			"Marie-Catherine de Marneffe",
			"Ivan Vladimir Meza Ruiz"
		],
		"booktitle": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
		"month": "July",
		"year": 2022,
		"address": "Seattle, United States",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2022.naacl-main.436/",
		"doi": "10.18653/v1/2022.naacl-main.436",
		"pages": "5959--5969",
		"abstract": "Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman's correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",
		"conference": "NAACL",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "misc",
		"id": "zouhar2022knowledgebaseindexcompression",
		"title": "Knowledge Base Index Compression via Dimensionality and Precision Reduction",
		"authors": [
			"Vilém Zouhar",
			"Marius Mosbach",
			"Miaoran Zhang",
			"Dietrich Klakow"
		],
		"year": 2022,
		"eprint": "2204.02906",
		"archivePrefix": "arXiv",
		"primaryClass": "cs.IR",
		"url": "https://arxiv.org/abs/2204.02906",
		"abstract": "Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100× compression with 75%, and (2) 24× compression with 92% original retrieval performance.",
		"conference": "SpaNLP workshop @ ACL",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "zouhar2021artefact",
		"title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access",
		"authors": [
			"Vilém Zouhar",
			"Marius Mosbach",
			"Debanjali Biswas",
			"Dietrich Klakow"
		],
		"booktitle": "Workshop on Commonsense Reasoning and Knowledge Bases",
		"year": 2021,
		"url": "https://openreview.net/forum?id=9_oCNR6R9l2",
		"abstract": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of *artefacts* (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are *fused* into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
		"conference": "CSKB workshop @ AKBC",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "misc",
		"id": "abdullah2021acousticwordembeddingscapture",
		"title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study",
		"authors": [
			"Badr M. Abdullah",
			"Marius Mosbach",
			"Iuliia Zaitova",
			"Bernd Möbius",
			"Dietrich Klakow"
		],
		"year": 2021,
		"eprint": "2106.08686",
		"archivePrefix": "arXiv",
		"primaryClass": "cs.CL",
		"url": "https://arxiv.org/abs/2106.08686",
		"abstract": "Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs.",
		"conference": "Interspeech",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "mosbach2021on",
		"title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
		"authors": ["Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow"],
		"year": 2021,
		"booktitle": "International Conference on Learning Representations",
		"url": "https://openreview.net/forum?id=nzpLWnVAyah",
		"abstract": "Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online",
		"conference": "ICLR",
		"code": "https://github.com/uds-lsv/bert-stable-fine-tuning",
		"blogpost": "None",
		"award": "None",
		"selected": true
	},
	{
		"type": "inproceedings",
		"id": "mosbach-etal-2020-closer",
		"title": "A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English",
		"authors": [
			"Marius Mosbach",
			"Degaetano-Ortlieb, Stefania",
			"Krielke, Marie-Pauline",
			"Badr M. Abdullah",
			"Dietrich Klakow"
		],
		"year": 2020,
		"month": "dec",
		"booktitle": "Proceedings of the 28th International Conference on Computational Linguistics",
		"address": "Barcelona, Spain (Online)",
		"publisher": "International Committee on Computational Linguistics",
		"url": "https://aclanthology.org/2020.coling-main.67/",
		"doi": "10.18653/v1/2020.coling-main.67",
		"pages": "771--787",
		"abstract": "Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models' performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.",
		"conference": "COLING",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "mosbach-etal-2020-interplay",
		"title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers",
		"authors": [
			"Marius Mosbach",
			"Anna Khokhlova",
			"Michael A. Hedderich",
			"Dietrich Klakow"
		],
		"year": 2020,
		"month": "nov",
		"booktitle": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
		"address": "Online",
		"publisher": "Association for Computational Linguistics",
		"url": "https://aclanthology.org/2020.blackboxnlp-1.7/",
		"doi": "10.18653/v1/2020.blackboxnlp-1.7",
		"pages": "68--82",
		"abstract": "Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.",
		"conference": "EMNLP (Findings)",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": true
	},
	{
		"type": "misc",
		"id": "mogadala2020sparsegraphsequencelearning",
		"title": "Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation",
		"authors": ["Aditya Mogadala", "Marius Mosbach", "Dietrich Klakow"],
		"year": 2020,
		"eprint": "2007.06077",
		"archivePrefix": "arXiv",
		"primaryClass": "cs.CV",
		"url": "https://arxiv.org/abs/2007.06077",
		"abstract": "Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",
		"conference": "Workshop on Bridge Between Perception and Reasoning @ ICML",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": false
	},
	{
		"type": "inproceedings",
		"id": "bizzoni-etal-2019-steps",
		"title": "Some steps towards the generation of diachronic WordNets",
		"authors": [
			"Yuri Bizzoni",
			"Marius Mosbach",
			"Dietrich Klakow",
			"Stefania Degaetano-Ortlieb"
		],
		"year": 2019,
		"month": "sep-oct",
		"address": "Turku, Finland",
		"publisher": "Linköping University Electronic Press",
		"url": "https://aclanthology.org/W19-6106/",
		"pages": "55--64",
		"conference": "NoDaLiDa",
		"editor": ["Mareike Hartmann", "Barbara Plank"],
		"abstract": "We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": true
	},
	{
		"type": "misc",
		"id": "mosbach2019logitpairingmethodsfool",
		"title": "Logit Pairing Methods Can Fool Gradient-Based Attacks",
		"authors": [
			"Marius Mosbach",
			"Maksym Andriushchenko",
			"Thomas Trost",
			"Matthias Hein",
			"Dietrich Klakow"
		],
		"year": 2018,
		"eprint": "1810.12042",
		"archivePrefix": "arXiv",
		"primaryClass": "cs.LG",
		"url": "https://arxiv.org/abs/1810.12042",
		"abstract": "Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.",
		"conference": "Workshop on Security in Machine Learning @ NeurIPS",
		"code": "None",
		"blogpost": "None",
		"award": "None",
		"selected": true
	}
]
